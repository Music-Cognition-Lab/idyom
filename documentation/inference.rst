Inference
=========

The techniques described in this section explain how IDyOM can be used for inferring latent variables from compositions using supervised learning.
Examples of latent variables that might be hypothesized to underlie compositions are *meter*, *key*, or *style*. 
The techniques described here support a the specification of a limited class of generative models in which a composition is represented as a set of observed variables and one or more latent variables can be specified.
In this limited class of generative models, inference can be performed *online*, that is after processing each event.
Thus, inference can be used to support prediction of musical events.

In IDyOM, music is represented as a sequence of temporally ordered events, :math:`\textbf{events}_0^i = [\text{event}_0, \text{event}_1, \cdots, \text{event}_i]`.
We denote the set of latent variables used for prediction by :math:`\textbf{lvars}`.
Supported generative models can in general be described as follows:

.. math:: p(\textbf{events}_0^i, \textbf{lvars}) = p(\textbf{events}_0^i|\textbf{lvars}) p(\textbf{lvars})

The above equation can be factorized as follows.

.. math:: p(\textbf{events}_0^i, \textbf{lvars}) = p(\textbf{lvars}) p(\text{event}_0 | \textbf{lvars}) \prod_{j=1}^{i} p(\text{event}_j | \textbf{events}_0^{j-1}, \textbf{lvars})

In online inference we have an a-priori distribution over the latent variables, and we want to find the posterior distribution using
The a-prior distribution may be :math:`p(\textbf{lvars})` in case no events have been observed, or in may be the posterior distribution given events 0 to :math:`i - 1` :math:`p(\textbf{lvars}|\textbf{events}_0^{i-1})`.
Inference can be performed online in such a model as follows.

.. math:: p(\textbf{lvars}|\textbf{events}_0^i) = \frac{p(\text{event}_i | \textbf{events}_0^{i-1}, \textbf{lvars}) p(\textbf{lvars}|\textbf{events}_0^{i-1}) }{p(\text{event}_i | \textbf{events}_0^{i-1})}

Two concepts play key role in the generative models: latent variables and interpretation.
Interpretation, here, means transforming the musical surface into a representation that depends on the state of the latent variables.
Such interpretation is implemented with *abstract viewpoints*: viewpoints defined as a function of both an event-sequence and the state of the latent variables.

The hypothesized states of latent variables are called *latent-states*, and these latent states may be of different *categories*.
This difference is relevant to the likelihood model, :math:`p(\textbf{events}|\textbf{lvars})`: the parameters of the model depend only on the category of a latent state.
The likelihood of a latent state is determined by the probability of the sequence of events. 
The likelihood is based on event predictions generated by a full-fledged multiple viewpoint system, which may be based on abstract and normal viewpoints.

That is, for each unique category of :math:`\textbf{lvars}`, a single model is learned.

Category labels are, for convenience, assumed to be part of the event representation.
Categories for a latent variable representing meter could for example be derived from the :math:`\texttt{pulses}` and :math:`\texttt{bardur}` event attributes.
Methods that specialize on specific latent variables can be written to define how categories are derived from basic event attributes.

Interpretations are refinements of categories.
The parameters of the likelihood model do not depend on interpretation, interpretation, implemented by abstract viewpoints, depends on both category and interpretation.

It is possible for latent variables to only have on category and multiple interpretations.
Key, for example, could be defined with one likelihood model that describes how melodies develop across scale degrees.
The scale degrees themselves depend on in which key a composition is interpreted.
It is also possible for latent variables to have multiple categories but no interpretations.
Style inference, for example, can be implemented by learning one model of melodic intervals per style.

The class supported generative models is restricted: it is assumed that each latent variable has a single true value per composition.
That is, there are no transition probabilities for :math:`lvars`.

Custom latent variables and abstract viewpoints can be defined with macros provided in the LATENT-VARIABLES and VIEWPOINTS modules. 
The sections below describe these modules in more detail.

Latent variables
----------------

Latent variables can represent analytical properties of a piece of music that are not directly observable but can be inferred from observable events.
The implementation of latent variables in IDyOM supports discrete latent variables for which a single value, or *state*, is assumed to underlie a composition. 
Each state of a latent variable is uniquely specified by the values of a set of parameters.
These parameters are split between two types: *category parameters*, and a set of *interpretation parameters*.
Any instantiation of the category and interpretation parameters is called a *latent state*, whilst any instantation of just the category parameters is called a *category*.

There is an important distinction in categories are used


The set of possible categories is determined by training data.

Events -> latent category
GET-EVENT-CATEGORY

Category -> interpretations

Parameter names and values -> category
CREATE-CATEGORY

During inference, a posterior is derived over the latent states of each participating latent variables.

Latent variables may be defined either without category parameters or without interpretation parameters.
The effect defining a latent variable without interpretation parameters is that each unique latent state will use a different predictive model.
Defining a latent variable without category parameters will ensure that the same predictive model is used for each latent state, inference must rely entirely on the effects of interpretation through abstract viewpoints.

Category parameters are typically event attributes that will be hidden during inference.
Interpretation parameters may or may not be event attributes.
The set of interpretation of parameters and the set of category parameters should be disjoint.

During inference, a prior distribution will be estimated for each latent variable.
The prior distributions are based on a set of training items labelled with categories.
The parameters of the prior distribution correspond to the latent states of the latent variable.
By default, prior distributions are estimated by counting the relative frequency of each category in the training data, but custom behaviors may be defined by writing specialized GET-PRIOR-DISTRIBUTION methods.

Latent variables may be *linked* together, which means that their posterior distribution will be inferred jointly.
Linked latent variables are represented by their own class, on which methods may specialize.
A linked latent variable consists of a number of constituent links, which are simply the latent-variables that it links together.
A linked latent variable derives its category, interpretation, and latent-state parameters from its constituent links by concatenating their parameters and sorting them alphabetically.

* Custom prior distributions for linked latent variables is not supported yet.

The prior distribution of a linked latent variable is calculated by calculating the joint distribution of its constituent links.
By default, independence is assumed among these prior distribution, hence the paramaters of the joint distribution are derived from the cartesian product of the parameters of the constituent prior distributions.

During inference, any combination of latent variables and linked latent variables may be provided to the inference engine to be inferred from the musical surface.
It may be the case that latent variables become probabilistically dependent through linking. 
For example, latent variables A and B are linked and latent variables B and C are linked and both are specified to be inferred for prediction, A, B, and C will become probabilistically dependent the system will infer a joint distribution over a single linked latent variable consisting of constituents A, B, and C.

Abstract viewpoints
-------------------

Abstract viewpoints are variants of derived viewpoints where some of the basic attributes in their typeset are replaced by values from the latent variable state.
Additionally, supplementary parameters may be taken from the latent variabel state that represent attributes to be inferred which would otherwise 
Abstract viewpoints are viewpoints of both an event sequence and the *latent variable state*.
The latent variable state contains the currently hypothesized value of any number of latent variables.
Abstract viewpoints may take as their function arguments parameters from  
The latent variable state is represented by a dynamic variable, \*LATENT-VARIABLE-STATE\*, which stores the current latent state of one or more latent variables.

Defining abstract viewpoints
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The macro DEFINE-ABSTRACT-VIEWPOINT can be used to define abstract viewpoints.
Like derived viewpoints, abstract viewpoint require NAME, TYPESET and event class specializer parameters in their definition.
Additionally, abstract viewpoints require a set of *event-attribute parameters* and *interpretation parameters*.
Event-attribute parameters represent event attributes that the viewpoint abstracts away from and must be actual basic attributes of the chosen event class.
Interpretation parameters specify any other parameters required by the abstract viewpoint that are available in the latent variable state, but not part of the event representation.

Like other viewpoints, abstract viewpoints rely on a viewpoint function.
For abstract viewpoints, this function should be a higher-order function that, given the parameters extracted from the latent variables states, returns a regular viewpoint function that performs the desired interpretation.
For example, an abstract viewpoint that returns the sequence of scale degrees of a melody based on a given key and mode should be defined using a higher-order function which takes a key and mode as arguments and returns a function that interprets event sequences in that key and mode.

The DEFINE-ABSTRACT-VIEWPOINT macro creates a normal viewpoint using the DEFINE-VIEWPOINT macro, but wraps the provided viewpoint function in a function that extracts the provided event-attribute and interpretation parameters from the latent variable state and passes the former as positional arguments and the latter as keyword arguments to the viewpoint function.

For interpretation parameters, a default value should be specified which is used when the viewpoint is used in the training phase (see below).

Importantly, event-attribute and interpretation parameters are not the same as categories and interpretations.
Categories are derived from event attributes and need not correspond to actual event attributes.
They specify when a different model should be used.
Interpretation, implemented by abstract viewpoints, may be independent of some aspects of categories 
While category parameters are typically actual event attributes, they are not required to be. 
The event attributes of abstract viewpoints on the other hand necessarily need to be event attributes.
The parameters are any remaining parameters that are required for interpretation, but are not event attributes (for example the phase of a metrical interpretation).
The *raison d'etre* of these remaining parameters (such as phase of a metre), is that certain aspects that one may want to infer are implicitly encoded in the event representation.
The absolute onset times in the MELODY representation, for example, implicitly encode information about the possible presence of an anacrusis since they are defined such that time 0 corresponds to the downbeat of the first bar.
This distinction---which may seem pointlessly confusing---has a technical reason that is related to the automatic generation of *training viewpoints*.
When an abstract viewpoint is defined, a training viewpoint (whose name should be provided to the DEFINE-ABSTRACT-VIEWPOINT macro) is automatically created.
A training viewpoint is used to train the predictive model for an abstract viewpoint.
This can be done simply by applying the training viewpoint to a set of training sequences like one would with any normal viewpoint (although the training sequences should be limited to a specific category for which a predictive model is being trained) and learning a predictive model from the resulting sequences.
The training viewpoint calls the same function that is used by the abstract viewpoint, but rather than sourcing the values of the event attributes from the latent state, they are sourced directly from the event representation.
Since the remaining parameters represent features that are implicitly encoded in the event representation, they should have a default value that can be assumed during training.

Generative multiple viewpoint systems
-------------------------------------

Three additional classes of multiple viewpoint systems are defined to support inference.
The most central of these is the abstract multiple viewpoint system, ABSTRACT-MVS.
With some exceptions, an abstract multiple viewpoint system appears to other functions and methods to behave exactly like a normal multiple viewpoint system.
The twist is that its behavior depends on the current \*LATENT-VARIABLE-STATE\*.

During inference, latent variables provided to be inferred for predictions are grouped together into independent *generative systems*. 
A generative system is a group of latent variables whose posterior distribution needs to be inferred jointly.
For example, if we specify latent variables :math:`(A B)`, :math:`C`, :math:`(A C)` and :math:`D` to be inferred (where variables grouped by brackets are linked together to be inferred jointly), two independent generative systems will be created:
One will jointly infer latent variables :math:`A`, :math:`B` and :math:`C` by linking latent variables :math:`A`, :math:`B`, and :math:`C` together.
Another will infer latent variable :math:`D`.
The user could of course have anticipated this transformation and have specified :math:`(A B C)` and :math:`D` to be inferred, but the system can take care of this reasoning as well.

ABSTRACT-MVS is initialized with a short-term model, a long-term model, a list of basic viewpoints, a list of viewpoints, a (possibly linked) latent variable and individual latent variables, one for each viewpoint.
ABSTRACT-MVS should be initialized with the MAKE-MVS function, which takes care of initializing its fields properly.
While a normal mvs stores long- and short-term models as a VECTOR of PPM models, one for each viewpoint, an abstract mvs needs to store considerably more models: per viewpoint, one model for each category needs to be stored.
This models are stored in the mvs-ltm and mvs-stm class slots, but instead of VECTORs, these slots hold hash tables where each model can be accessed by its latent-variable attribute and category.

An abstract mvs achieves its dependence on the latent state by overriding the MVS-LTM and MVS-STM slot acccessor methods.
The LTM and STM accessor methods return a vector of models, with one model for each viewpoint of the mvs.
However, in an abstract mvs, each viewpoint is associated with a latent variable.
Which models are returned depends on the latent category of each latent variable as encoded in the current latent state.
A generative system with latent variables

Prediction 

Inference and prediction
------------------------

Latent variable inference is fully integrated into the IDyOM top-level function.
In order to use it, 





